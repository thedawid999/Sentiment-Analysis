{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "906218e0-2593-4554-98da-62927600bb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\dawid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dawid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dawid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dawid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "NEGATIONS = {\"not\", \"no\", \"nor\", \"n't\", \"never\", \"hardly\", \"barely\"}\n",
    "STOPWORDS = STOPWORDS - NEGATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dfbf146e-ec24-4935-96ea-9351495b0587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Title:  not the best i thought\n",
      "Review:  im not liking this kind of shit better be cheaper or i wont be willing to buy it\n"
     ]
    }
   ],
   "source": [
    "title = input(\"Title: \")\n",
    "review = input(\"Review: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec0473-1a9e-433f-842a-1eeece68024d",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64023e8-4697-428e-9662-950d8951a10a",
   "metadata": {},
   "source": [
    "### 1. Combining Title and Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5dbe2784-daca-47ea-859f-12077e3f5cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = title + \" \" + review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda2495-5354-403b-bb28-ae03f9eca6c8",
   "metadata": {},
   "source": [
    "### 2. Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b1775c7-13cf-4ccb-8090-c5a2816e877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74ab45-77fe-489f-93fb-d70052cf2c88",
   "metadata": {},
   "source": [
    "### 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b3843224-9900-478f-b082-bb2365d3667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'the', 'best', 'i', 'thought', 'im', 'not', 'liking', 'this', 'kind', 'of', 'shit', 'better', 'be', 'cheaper', 'or', 'i', 'wont', 'be', 'willing', 'to', 'buy', 'it']\n"
     ]
    }
   ],
   "source": [
    "data = nltk.word_tokenize(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d22660-c7a3-4556-987a-7fd3c2f31d23",
   "metadata": {},
   "source": [
    "### 4. Removing stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a07889c-faa8-487d-92d2-f5e54b02180e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'best', 'thought', 'im', 'not', 'liking', 'kind', 'shit', 'better', 'cheaper', 'wont', 'willing', 'buy']\n"
     ]
    }
   ],
   "source": [
    "data = [t for t in data if t not in STOPWORDS and t not in string.punctuation]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aec4ea-cbe4-4f76-babf-a579b064785e",
   "metadata": {},
   "source": [
    "### 5. Removing emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd51065f-065d-49e6-a23c-cd8bad1c165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\n",
    "    \"[\"\n",
    "    \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "    \"]+\",\n",
    "    flags=re.UNICODE\n",
    ")\n",
    "\n",
    "def remove_emojis_from_list(token_list):\n",
    "    if isinstance(token_list, list):\n",
    "        return [emoji_pattern.sub(\"\", token) for token in token_list]\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "416bf813-3a1c-47f2-b2fd-bf25a2c3da6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'best', 'thought', 'im', 'not', 'liking', 'kind', 'shit', 'better', 'cheaper', 'wont', 'willing', 'buy']\n"
     ]
    }
   ],
   "source": [
    "data = remove_emojis_from_list(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b585dd82-caf0-4349-af02-45a209d5e43e",
   "metadata": {},
   "source": [
    "### 6. Lemmatization + POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d80a0df-a132-4da8-bcb0-faadeea01538",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7deb2421-e115-4b17-952a-5e93763f289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass word-type in WordNetLemmatizer-Format\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c8321b2-0997-4338-bd18-be3f70358cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize with a specific word-type\n",
    "def lemmatize_tokens(tokens):\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    return [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "        for word, pos in pos_tags\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2054dfa5-dab4-442e-a4a3-2a81f96087c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'best', 'think', 'im', 'not', 'like', 'kind', 'shit', 'well', 'cheap', 'wont', 'willing', 'buy']\n"
     ]
    }
   ],
   "source": [
    "data = lemmatize_tokens(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe6610-ce3b-4df0-9965-94664e641a0b",
   "metadata": {},
   "source": [
    "### 7. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d788ed6-9580-4841-a168-ef8201be9035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
