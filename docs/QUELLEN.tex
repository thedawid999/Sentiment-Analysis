[1] https://lena-voita.github.io/nlp_course/text_classification.html#main_content
[2] https://amazon-reviews-2023.github.io/data_processing/0core.html
[3] https://web.stanford.edu/~jurafsky/slp3/
[4] https://www.qualtrics.com/de/erlebnismanagement/marktforschung/sentiment-analysis/
[5] https://mindsquare.de/knowhow/sentimentanalyse/
[6] https://blog.jetbrains.com/de/pycharm/2025/06/einfuehrung-in-die-sentimentanalyse-mit-python/


[2]
0-core:
    - vollständiger Rohdatensatz ohne Vorfilterung
    - VORTEIL: maximale Vielfalt und Realismus 
    - NACHTEIL: enthält Spam, Fake-Reviews, User mit nur 1 Review, Produkte mit
    wenigen Bewertungen. Hoher Rechenaufwand und schlechte Qualität
5-core:
    - nur User und Produkte, die mind. 5 Bewertungen haben
    - NACHTEIL: repräsentiert keine reale Verteilung, Verlust von Kontext
    - VORTEIL: höhere Qualität, weniger Rauschen, weniger Daten also schneller
    zum Trainieren und besser geeignet für ein Studiumprojekt

    
[1] 
"A classification model can be either generative or discriminative.
- generative: based on probability distribution
- discriminative: learns borders between classes"

[1] 
"Naive Bayes ist generative. Die Annahmen sind:
- Wortfolge ist egal
- Merkmale (also die Wörter) sind unabhängig voneinander wenn die Klasse
bekannt ist {Das Auftreten von „good“ist unabhängig vom Auftreten von „very“,
wenn wir wissen, dass der Text positiv ist.} 
In Wirklichkeit ist es nicht immer der Fall, deswegen NAIVE
    
Daten müssen kategorial sein, also diskret. Meistens ist die Repräsentation
ein BoW-Vektor, ale es sind Zählwerte von Wörtern
    
einfach, schnell, aber unrealistische Unabhängigkeitsannahmen"

[1] 
"Logistic Regression ist discriminative. 

Merkmale müssen nicht kategorial sein. Man kann BoW nutzen oder was anderes.
Hier wird jedes Feature mit einem Gewicht multipliziert und anschließend
ein Bias addiert. Die Klassenwahrscheinlichkeit enthält man durch 
[Normalisierung mit Softmax (Aktivierungsfunktion für multinomiale LR). Dies 
ergibt einen K-dimensionalen Vektor (Wahrscheinlichkeitsverteilung für jede 
Klasse K)] [3, Chapter 4]

Keine starken Annahmen wie bei Naive Bayes.

einfach, keine Unabhängigkeitsannahmen, aber nicht so schnell"

    
[3, Chapter 4] 
Typische Featuers sind
- Häufigkeit positvier/negative Wörter (z.B. "awesome" vs "awful")
- Vorhandensein von Negationen (z.B. "no")
- Satzzeichen (z.B. Ausrufezeichen) und Textlänge

Cross-Entropy-Loss: Verlustfunktion, Abstand zwischen vorhergesagten
    Wahrscheinlichkeiten und dem tatsächlichen Label, Ziel: Minimierung
Stochastisches Gradient Descent: Verfahren, um die Gewichte schrittweise so
    anzupassen, dass das Minimum der Verlustfunktion erreicht wird
Regularisierung: Um Overfitting zu vermeiden werden große Gewichte durch
    einen Strafterm in der Verlustfunktion bestraft.

Bewertungsmetriken: Precision, Recall, F1-Score
-> Accuracy ist bei ungleich verteilten Daten kein guter Indikator


[3, Chapter 5]
Embeddings: ein allgemeiner Begriff für Vektordarstellungen von Wörtern, die
    semantische Ähnlichkeiten kodieren. Ähnliche Wörter liegen näher zueinander
    Hauptsächlich für neuronale Netze 
    

[3, Chapter B]
NAIVE BAYES 
- Laplace-Glättung (Add-one smoothing): um zu verhindern, dass die Wahrscheinlichkeit
für ein Wort, das nicht im Trainingsset vorkam, Null wird, was die gesamte 
Berechnung zerstören würde
- Binarisierung: Bei der Sentimentanalyse ist es oft wichtiger, OB ein Wort 
vorkommt, als wie oft. Hierbei werden Wortzählungen auf 1 begrenzt, was die 
Performance oft verbessert
- Umgang mit Negation: Eine einfache, effektive Methode ist das Voranstellen 
eines Präfixes (z. B. NOT_) vor jedes Wort nach einer Verneinung 
(wie „nicht“, „kein“) bis zum nächsten Satzzeichen 
(z. B. „didn’t like this movie , but I" -> "didn’t NOT_like NOT_this NOT_movie , but I")

Signifikanztests: Um zu beweisen, dass ein Modell wirklich besser ist als ein 
anderes, sollten statistische Tests wie der Bootstrap-Test durchgeführt werden

Sentiment-Lexika: Wenn wenig Trainingsdaten vorhanden sind, können 
vordefinierte Listen positiver und negativer Wörter (z. B. MPQA) als Merkmale 
genutzt werden.


[3, Chapter 22]
- Valenz-Dimension: Sentiment wird oft über die Dimension der Valenz definiert. 
In der Analyse von Rezensionen ist dies die Messung, wie sehr eine Person ein 
Produkt mag oder ablehnt.
- Arten von Sentiment-Lexika:
    - Binär: Einfache Listen für positive und negative Wörter 
    (z. B. MPQA, Hu und Liu)
    - Intensitätsbasiert: Lexika, die Wörter auf Skalen (z. B. 0 bis 1) für 
    Dimensionen wie Valenz, Erregung und Dominanz bewerten 
    (z. B. NRC VAD Lexicon)
- Typische Merkmale in Rezensionen:
    - 1-Sterne-Rezensionen: Nutzen häufig logische Negationen (no, not) und 
    beziehen sich oft auf Personen (z. B. manager, waiter)
    - 5-Sterne-Rezensionen: Nutzen verstärkt Emphatika und Universalbegriffe 
    (very, highly, always)
- Feature-basierte Ansätze: In überwachten Modellen (wie logistischer 
Regression) dienen Wortzählungen aus Lexika als zusätzliche Merkmale, um die 
Genauigkeit zu verbessern


[3, Chapter H]
Kernanforderungen an das System:
    - Eindeutigkeit: Während Rezensionen sprachlich mehrdeutig sein können, 
    muss die interne Repräsentation der Meinung absolut eindeutig sein
    - Kanonische Form: Verschiedene Formulierungen, die dasselbe ausdrücken 
    (z. B. „Der Akku ist schwach“ vs. „Die Batterie hält nicht lange“), 
    sollten idealerweise auf dieselbe formale Repräsentation abgebildet 
    werden, um die Auswertung zu vereinfachen
    - Inferenz: Ein System sollte in der Lage sein, Wissen abzuleiten, das 
    nicht explizit im Text steht. Beispiel: Wenn ein User schreibt 
    „Ich brauchte eine Rückerstattung“, kann das System auf ein negatives 
    Erlebnis schließen.


[4]
- Sentimentanalyse (auch Stimmungsanalyse) ist der Prozess zur Ermittlung von 
Meinungen, Urteilen oder Emotionen hinter einem Text.
- Das Ziel ist es, den Ausdruck von Sprache dahingehend zu bewerten, in welchem 
Ausmaß er positiv, negativ oder neutral ist.
- Häufig erfolgt die Bewertung über einen Sentiment Score, beispielsweise auf 
einer Skala von -1 (negativ) bis +1 (positiv)

HERAUSFORDERUNGEN
- Kontextabhängigkeit: Ein Wort kann je nach Produkt positiv oder negativ sein 
(z. B. ein „dünner“ Laptop ist positiv, „dünne“ Wände im Hotel sind negativ)
- Sprachliche Nuancen: Sarkasmus, Ironie, Redewendungen und grammatikalische 
Komplexität sind für KI-Tools schwer zu interpretieren.
- Subjektivität: Was eine Person als positiv empfindet, kann für eine andere 
neutral wirken.


[5]
- Das Ziel ist die automatische Auswertung von Texten, um Aussagen als positiv, 
neutral oder negativ einzustufen und die dahinterliegende Haltung oder 
Stimmung zu erkennen.
- Die Analyse kann auf drei Ebenen erfolgen: einzelne Ausdrücke, Sätze oder 
das gesamte Dokument

VORTEIL VON ML GEGENÜBER REGELBASIERT
Es erkennt übergeordnete Sinnzusammenhänge, ist fehlertolerant gegenüber 
Tippfehlern und kann unbekannte Ausdrücke anhand der Tonalität gewichteter 
Sentiments einschätzen

SENTIMENTANALYSE - ANWENDUNGEN
- Produktverbesserung: Durch die Analyse lässt sich messen, wie Veränderungen 
an Produkten wahrgenommen werden, um Fehler schnell zu beheben.
- Support-Priorisierung: Stark negative Rezensionen können automatisch erkannt 
und nach Dringlichkeit sortiert werden, um kritische Anfragen schneller zu 
bearbeiten.
- Marktforschung: Identifikation von Stärken und Schwächen im Vergleich zur 
Konkurrenz.


[6]
- Sentimentanalyse: Die Untersuchung des emotionalen Tons eines Textes, wobei meist 
die Valenz oder Polarität (wie positiv oder negativ eine Stimmung ist) bestimmt wird

ARTEN
- Binär: Einteilung in zwei Klassen wie positiv oder negativ.
- Abgestuft: Verwendung einer Likert-Skala (z. B. SST-5 mit fünf Stufen von sehr 
positiv bis sehr negativ).
- Kontinuierlich: Messung über Zahlenwerte, z. B. von -1 (stark negativ) bis 1 
(stark positiv).
- Subjektiv vs. Objektiv: Unterscheidung, ob ein Text eine persönliche Haltung 
ausdrückt oder rein sachlich ist

METHODISCHE ANSÄTZE
- Lexikonbasiert (z. B. VADER, Pattern): Nutzt Wortlisten mit festen Sentimentwerten. 
Diese Methoden sind schnell und präzise bei Einzelbegriffen, erfordern aber oft 
manuelle Erstellung.
- Machine Learning (z. B. Naive Bayes): Ein Klassifikator wird mit gelabelten 
Datensätzen (z. B. Filmkritiken) trainiert. Er ist schnell, kann aber bei komplexen 
Texten mit Negationen (Verneinungen) Schwierigkeiten haben.
- Große Sprachmodelle (LLMs / Transformers): Diese können komplexe Beziehungen und 
Kontext im Text modellieren, sind jedoch in der Verarbeitung langsamer

WICHTIGE PYTHON-TOOLS
- VADER (aus NLTK): lexikonbasiert, spezialisiert auf soziale Medien und Kurztexte.
    --> ABER SELBST TRAINIEREN, EIGENES MODELL ERSTELLEN!
- TextBlob: lexikonbasiert, bietet sowohl lexikonbasierte als auch Naive-Bayes-Ansätze.


[keine Quelle]
- Embeddings eher für neuronale Netze (auch für LR), besser LR {TF-IDF oder BoW}
und Naive Bayes {TF-IDS oder BoW}
- OneHot Vektoren sind hochdimensionale Vektoren, die jedes Wort eindeutig
kodieren, aber keine semantische Informationen speichern
